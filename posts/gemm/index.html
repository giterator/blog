<!doctype html><html lang=en dir=auto data-theme=light><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Designing a Performant CUDA GEMM Kernel from First Principles | Pranav Venkatram</title><meta name=keywords content><meta name=description content="Introduction
In this post, I explore how to implement a performant CUDA GEMM kernel from scratch for NVIDIA GPUs. The goal is to design an efficient kernel grounded in hardware features and first principles, such that its performance characteristics are explainable and achieves performance close to cuBLAS. I examine how the problem naturally decomposes into each level of hardware abstraction (SM, warp, thread) and how to maximize performance at each level through effective use of the memory hierarchy. We start at the lowest level of abstraction (thread) and work our way up to the highest level (SM)."><meta name=author content><link rel=canonical href=https://giterator.github.io/blog/posts/gemm/><link crossorigin=anonymous href=https://giterator.github.io/blog/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://giterator.github.io/blog/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://giterator.github.io/blog/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://giterator.github.io/blog/favicon-32x32.png><link rel=apple-touch-icon href=https://giterator.github.io/blog/apple-touch-icon.png><link rel=mask-icon href=https://giterator.github.io/blog/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://giterator.github.io/blog/posts/gemm/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"&&(document.querySelector("html").dataset.theme="dark")</script><meta name=google-site-verification content="1LLFqlvsnXMWQkeMLmEpwtvLkQPReiC-aOqmM9He7bY"><meta property="og:url" content="https://giterator.github.io/blog/posts/gemm/"><meta property="og:site_name" content="Pranav Venkatram"><meta property="og:title" content="Designing a Performant CUDA GEMM Kernel from First Principles"><meta property="og:description" content="Introduction In this post, I explore how to implement a performant CUDA GEMM kernel from scratch for NVIDIA GPUs. The goal is to design an efficient kernel grounded in hardware features and first principles, such that its performance characteristics are explainable and achieves performance close to cuBLAS. I examine how the problem naturally decomposes into each level of hardware abstraction (SM, warp, thread) and how to maximize performance at each level through effective use of the memory hierarchy. We start at the lowest level of abstraction (thread) and work our way up to the highest level (SM)."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-01-01T09:17:17-06:00"><meta property="article:modified_time" content="2026-01-01T09:17:17-06:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Designing a Performant CUDA GEMM Kernel from First Principles"><meta name=twitter:description content="Introduction
In this post, I explore how to implement a performant CUDA GEMM kernel from scratch for NVIDIA GPUs. The goal is to design an efficient kernel grounded in hardware features and first principles, such that its performance characteristics are explainable and achieves performance close to cuBLAS. I examine how the problem naturally decomposes into each level of hardware abstraction (SM, warp, thread) and how to maximize performance at each level through effective use of the memory hierarchy. We start at the lowest level of abstraction (thread) and work our way up to the highest level (SM)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://giterator.github.io/blog/posts/"},{"@type":"ListItem","position":2,"name":"Designing a Performant CUDA GEMM Kernel from First Principles","item":"https://giterator.github.io/blog/posts/gemm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Designing a Performant CUDA GEMM Kernel from First Principles","name":"Designing a Performant CUDA GEMM Kernel from First Principles","description":"Introduction In this post, I explore how to implement a performant CUDA GEMM kernel from scratch for NVIDIA GPUs. The goal is to design an efficient kernel grounded in hardware features and first principles, such that its performance characteristics are explainable and achieves performance close to cuBLAS. I examine how the problem naturally decomposes into each level of hardware abstraction (SM, warp, thread) and how to maximize performance at each level through effective use of the memory hierarchy. We start at the lowest level of abstraction (thread) and work our way up to the highest level (SM).\n","keywords":[],"articleBody":"Introduction In this post, I explore how to implement a performant CUDA GEMM kernel from scratch for NVIDIA GPUs. The goal is to design an efficient kernel grounded in hardware features and first principles, such that its performance characteristics are explainable and achieves performance close to cuBLAS. I examine how the problem naturally decomposes into each level of hardware abstraction (SM, warp, thread) and how to maximize performance at each level through effective use of the memory hierarchy. We start at the lowest level of abstraction (thread) and work our way up to the highest level (SM).\nSimon Boehm’s worklog is the primary source of inspiration for this post. My intent is to reproduce his results, document my learnings and extract generic design principles that can be applied to other kernels. While several of these principles such as coalesced, vectorized global memory access and shared memory conflicts are well documented, my main takeaway has been a deeper understanding of warp tiling, particularly with respect to shared memory multicast behavior and register file cache reuse enabled by improved instruction scheduling.\nPeek at Final Results The kernel was run on a Tesla T4 GPU for a variety of matrix sizes and the performance relative to cuBLAS is shown below:\nMatrix Size cuBLAS GFLOPs/s My GFLOPs/s % of cuBLAS Performance Achieved 256 697.015 118.654 17.02 512 2062.63 485.039 23.52 1024 2657.68 2662.89 100.20 2048 4447.76 3944.75 88.69 4096 4312.22 4446.76 103.12 8192 4039.70 4151.41 102.77 For larger matrices, our implementation runs faster than cuBLAS on the T4. For smaller matrices, cuBLAS is significantly faster as it selects kernels optimized for smaller problem sizes.\nAspects of a CUDA Kernel A kernel generally comprises of two components:\nCompute (Dot product for GEMM) GPU compute hardware consists of a large number of threads organized in a hierarchical fashion. Each level of hardware has a corresponding software abstraction that executes on it:\nA kernel is designed to decompose a compute problem in a hierarchical fashion such that each thread computes some portion of the output. Memory Transactions (Load and Store) Due to the memory wall, it is essential to hide the latency of loads and stores by overlapping them with compute. Maximizing data reuse at each level of the memory hierarchy mitigates the overall load latency. Data is brought from slower memory to faster memory fewer times and repeatedly accessed from faster memory. In a GPU kernel, the loading process follows this procedure:\nLoad from Global Memory (DRAM) to Shared Memory (User Managed L1 cache) Load from Shared Memory to Registers Compute and accumulate result in Registers Store from Registers to Global Memory (We don’t cache in shared memory since the data won’t be reused) Shared Memory is private to each SM and Registers are private to each thread. We can ensure data reuse at the thread block level by loading data into shared memory and reusing it across threads in the block and at the register level by computing multiple outputs per thread (multiple outputs require the same rows/columns). L2 cache and Global Memory are accessible across all SMs. Since L2 cache cannot be managed explicitly and the highest level of software abstraction is a thread block (maps to an SM), we will focus on data reuse at the level of shared memory (thread block level) and registers (thread level).\nDesigning the GEMM Kernel Cm,n = Am,k x Bk,n Identifying the independent operation Each output element is computed by performing a dot product of a row from matrix A and a column from matrix B i.e. each element in C is computed independently of others.\nFirst thoughts Since each element of C is computed independently, we can have each output element computed by a different thread. This would involve each thread loading 1 row of A and 1 column of B from Global memory and computing the dot product.\nHowever, due to the memory wall, particularly for large matrices, the latency of loading data from Global memory outweighs the compute time. Further, output elements residing in the same row require the same row of A and different columns of B. Likewise, output elements residing in the same column require the same column of B and different rows of A. The current approach involves reloading the same data for computing multiple outputs.\nMemory latency can be mitigated by loading data once and reusing it to compute multiple outputs.\nData reuse As mentioned earlier, the GPU memory hierarchy offers 2 levels of caching we can control: Shared Memory (L1) and Registers.\nSince Registers are the fastest memory available, we should try to maximize data reuse at this level. As registers are private to each thread, we can only achieve data reuse at the register level by computing multiple outputs per thread. Each thread is thus responsible for computing a tile of the output matrix by reusing rows of A and columns of B cached in the registers.\nThere are 2 approaches to tiling:\n1D output tile 2D output tile Let’s assume that n = m = k i.e. matrices A, B and C are square matrices of size n x n.\nAs shown above, a square 2D tile enables maximum data reuse. In general, a rectangular tile enables more reuse than a 1D tile but lesser reuse than a square tile.\nThe takeaway from this analysis is that we should use a 2D tile but the exact dimensions can be treated as parameters that can be tuned. Further, the number of FLOPs needed to compute a single output element is fixed (2n). Maximizing data reuse allows us to compute the same number of FLOPs with fewer loads, mitigating memory latency (greater arithemtic intensity).\nCurrent state of the kernel:\nEach thread computes a 2D output tile\nNow that we have determined what the lowest level of hardware abstraction should compute, we move to defining the higher levels of abstraction.\nThreads that compute tiles residing in the same row or the same column load the same rows of A or same columns of B from Global memory into their registers. These are redundant loads that stress the memory bus and waste bandwidth. Hence, we cache data in shared memory (L1) and reuse it across threads in the same thread block.\nSince we have determined that a 2D tile maximizes data reuse, the tile computed at each level of the software abstraction should be a 2D tile where tiles computed by lower levels of abstraction compose a larger tile at a higher level.\nWe define the terminology as follows:\nThreadtile: tile computed by 1 thread Warptile: tile computed by 1 warp (32 threads) Blocktile: tile computed by 1 thread block (multiple warps) Each of these tiles will have their own heights and widths. We use the following variable names for simplicity:\nThreadtile (height, width): TH x TW Warptile (height, width): WH x WW Blocktile (height, width): BH x BW Warptiling In hardware, a warp (group of 32 threads) is the fundamental execution unit. Before defining how threads in a warp are organized to compute a Warptile, we should first explore a key warp-specific hardware feature: Register File Cache (RFC).\nRegister File Cache \u0026 Instruction Scheduling There is little official documentation on the Register File Cache. Interestingly, researchers have reverse engineered GPU architectures, shedding light on these details. I referred to Analyzing Modern NVIDIA GPU cores to understand the RFC and instruction scheduling, which collectively seem to provide a motivation for warptiling.\nIn GPUs, the Register Files are much larger than the L1 cache. Since threads access registers repeatedly, there is a great deal of contention for the register file, resulting in stalls due to serialized access. As such, the GPU hardware contains a Register File Cache (RFC) that caches the most recently used registers. This exploits temporal locality such that repeated access of the same registers avoids going to the register file.\nAs seen in the diagram below, the RFC is island-specific i.e. register caching can only be exploited at the warp level (Note: the diagram refers to islands as sub-cores).\nSM Architecture (Modified from: Analyzing Modern NVIDIA GPU cores) From a scheduling point of view, the warp scheduler implements Fine Grained Multi Threading i.e. it determines which warp should issue an instruction every cycle. The scheduler prioritizes instructions from the existing warp provided it is ready to execute. If the current warp is stalled, the scheduler will switch to another warp to issue instructions and keep the execution units busy.\nConsidering the RFC and scheduler behaviour, a context switch to a different warp would cause a different set of registers (containing rows of A and columns of B) to be accessed, invalidating the RFC. Hence, to exploit this level of the memory hierarchy, we should try to keep the same warp active for as long as possible. To achieve this, the instruction buffer must contain an independent instruction from the same warp i.e. can be issued in the current cycle.\nOrganizing threads in a warp Having established the role of an individual thread and the hardware supporting warp execution, we can now define how threads in a warp are organized to compute a Warptile.\nSince each thread computes a TH x TW tile of C, a warp will compute 32 such tiles. At the thread level, we computed a 2D threadtile to enable data reuse at the register level. At the warp level, we organize these threadtiles in a 2D manner once again to enable reuse at the shared memory level. Let’s assume that the data to compute the blocktile is already loaded into shared memory. This 2D organization enables fewer rows and columns to be loaded from shared memory into the registers at a warp level since each thread now requires the same rows and columns as others. Since threads in a warp execute in lock-step, threads loading the same elements from shared memory will do so at the same time. GPU hardware is optimized to service such requests via Intra-warp multicast wherein a shared memory element is accessed once and sent to all requesting threads in the warp in the same time it would take for a single thread to load a single element. This effectively reduces the number of loads from shared memory to the registers at a warp level. Axel Feldmann’s wonderful benchmark showcases this among many others experiments.\nWe also want to ensure that a warp doesn’t get context switched and invalidate the RFC so we have a warp compute a WH x WW warptile that is larger than 32 threadtiles. A warp will initially compute a warp subtile (32 thread tiles) and then stride to compute the remaining warp subtiles within the same WH x WW warptile. This increases the amount of work done by a warp, thus increasing the warp level ILP. This ensures that the warp scheduler has instructions from the same warp to issue in subsequent cycles, keeping the RFC hot. Larger warptile sizes increase the amount of sequential work done by a warp. In the extreme case where the warptile size is too large, we will exceed register limits and perform too much sequential work, leaving SMs idle. In the other extreme, the warptile size is too small, resulting in frequent invalidations of the RFC. The warptile size must be chosen to strike a balance between these two considerations (done by autotuning).\nThe warptiles computed by each warp in a thread block compose a blocktile. The warptiles are organized in a 2D manner as well, resulting in a BH x BW blocktile.\nCurrent state of the kernel:\nWarptiling exploits two distinct hardware features using two levels of tiling:\n1. Warp subtile (32 thread tiles)\nThe 32 threadtiles are organized in a 2D manner, leveraging intra-warp multicast.\n2. Warptile (multiple warp subtiles)\nStriding to compute remaining warp subtiles within a larger warptile increases warp level ILP, keeping the RFC hot.\nExploiting Shared Memory Since shared memory is private to an SM, data can only be shared by threads in the same thread block. To compute a blocktile of C, the necessary rows of A and columns of B are loaded from global to shared memory. However, since the K dimension can be large, BH rows of A and BW columns of B will exceed the capacity of shared memory. Hence, we need to tile the inputs along the K dimension. The operation computed by the kernel is now a tiled inner product where each dot product is chunked and the partial results are iteratively accumulated.\nThe tiling approach for the inputs is now extended from: to: Shared Memory Layout The BH x BK tile of A and BW x BK tile of B are loaded from global memory into shared memory. The BH x BW tile of C is accumulated into registers iteratively and then stored to global memory. Each output element requires 1 row of BK elements from the tile of A and 1 column of BK elements from the tile of B. Hence, each threadtile will load multiple rows and columns of A and B into its registers.\nShared memory is organized into 32 banks where each element in a bank stores 4 bytes. Since we are operating on FP32 values, each element stores a single FP32 value. When reading from or writing to shared memory, if threads within the same warp access different elements in the same bank, a bank conflict will occur, resulting in serialized access. However, if threads within the same warp access the same element in the same bank, the element is accessed once and broadcasted so there is no performance penalty. Based on these 2 factors, we can design our data layout and access pattern to minimize bank conflicts and maximize intra warp multicast.\nAn array of floats is mapped to banks in shared memory as follows: We use 2 arrays in shared memory to store the tiles of A and B respectively.\nA tile in shared memory (BH x BK) For reasoning about the layout of A in shared memory banks, we assume that each column maps to the same bank i.e. BK is a multiple of 32. This is an oversimplification but it is helpful to reason about conflicts.\nIf the BH x BK tile of A is stored in row major order in shared memory, threads in a warp which need the same rows will exploit intra-warp multicast. However, threads that read different rows will conflict with each other as shown below:\nThis can be solved by storing the tile of A in column major order (transposed layout). Threads in a warp accessing the same column will still exploit multicast as before but threads accessing different columns will now access different banks, avoiding bank conflicts:\nB tile in shared memory (BK x BW) As with A, we assume that each column maps to the same bank i.e. BW is a multiple of 32. If we store the tile of B in row major order, threads in a warp which need the same columns of B will exploit intra-warp multicast. Threads that need different columns of B will access different bank, avoiding conflicts anyway.\nGlobal Memory loads and stores (Coalescing and Vectorization) Now that we have defined the compute and desired shared memory layout, we can define the approach to load the tiles of A and B from global memory into shared memory.\nEven though multiple blocktiles use the same rows/columns as others, we are forced to load them independently within each thread block. The data may reside in the L2 cache, enabling faster loads for subsequent blocks although the order of thread block execution is abstracted away from the programmer so we cannot assume this is the case.\nEach thread block will load a BH x BK tile of A and a BK x BW tile of B from global memory into shared memory. It is beneficial for warps to load contiguous data from global memory as the loads are coalesced into fewer transactions by the hardware. This also applies to stores.\nNote: Coalescing is a warp level optimization.\nFurther, vectorized load and store instructions improve performance by reducing the instruction issue overhead as fewer load/store instructions are executed. 4-wide vectorized instructions enable 4 floats to be loaded per instruction. At a hardware level, this still takes the same number of cycles as loading 1 float per instruction (shown in Axel Feldmann’s benchmark).\nTo exploit both coalescing and 4-wide vectorized loads, each thread in a warp loads 4 contiguous floats from global memory (A and B arrays). Hence, the threadtile width (TW) must be 4. All threads in the thread block then stride vertically to load the remaining elements from A and B tiles. The loaded elements are placed in the desired layout (row major or tranposed) in shared memory.\nOnce a thread has computed a threadtile, it writes the values back to global memory. Since we have organized threads in a warp to compute a 2D warptile and TW=4, each thread writes 4 contiguous elements of C to global memory and threadtiles on the same row have their stores coalesced.\nFinal GEMM Kernel Stages The final kernel implementation has specific stages separated by barriers where each thread plays a different role in each stage:\nSince threads cooperate within a thread block, the barriers ensure that all threads in a block reach the barrier before proceeding to the next stage.\nAutotuning The hierarchical kernel exposes parameters at each level of abstraction:\nThreadtile dimensions: TH (TW=4 already) Warptile dimensions: WH, WW Blocktile dimensions: BH, BW Tile size along reduction dimension (inputs): BK I ran an autotuning script that exhaustively sweeps over a range of values for each parameter and benchmarks the performance of each configuration. The configuration with the best performance was selected.\nTypically, we would like to write a generic kernel that can be used across GPU architectures rather than rewriting kernels every time a new architecture is released. Our gemm implementation is sufficiently generic that it can be deployed across different GPUs since we exploit common features supported by NVIDIA GPUs (provided the architecture is not vastly different). However, since different architectures have different memory bandwidths, cache sizes etc. the optimal tile sizes vary. Autotuning helps us treat the kernel as a black box and find the optimal tile sizes without having to analyze architectural differences.\nInteresting Observations GEMM on GPU and CPU are not so different As mentioned at the start of the post, a kernel consists of compute and memory transactions. In the fully implemented GEMM kernel of ~130 lines of code, the actual compute portion is only 1 line of code wrapped in 4 loops (totally ~10 lines of code). The bulk of the code handles data loading, caching in shared memory, populating registers and storing the result i.e. data movement.\nSince the lowest level of GPU hardware is a thread, the compute code is exactly the same as that of blocked matrix multiplication for single threaded CPU execution:\nfor(int c = 0; c \u003c w_hor; c++){ for(int r = 0; r \u003c w_vert; r++){ for(int tr = 0; tr \u003c TH; tr++){ for(int tc = 0; tc \u003c TW; tc++){ c_out[(r*TH + tr)*(w_hor*TW) + (c*TW + tc)] += reg_a[r*TH+tr] * reg_b[c*TW+tc]; } } } } On a GPU, this is scaled up to compute a larger output matrix using a larger number of threads.\nDynamic Indexing breaks register allocation The autotuned parameters resulted in some bank conflicts in shared memory. I resolved the conflicts through a combination of padding and modified access pattern. Despite conflict resolution, the kernel was still slower than the original version so I don’t cover it in this post. In the process of playing around with the access pattern, I came across an interesting compiler related nuance.\nI was loading elements from shared memory into an array that was small enough to fit into registers. Since the array size is known at compile time, I assumed that the compiler would assign registers to the array elements. It turns out that my modified indexing logic which utilizes a % operator causes the index to be determined at runtime. Even though the index always falls within the bounds of the array, this cannot be guaranteed at compile time. The compiler plays it safe and allocates the array in local memory, resulting in a ~20x drop in performance. This is effectively register spilling even though the array fits in registers and the index is always within bounds.\nInspecting the PTX, we see store instructions to local memory:\nWhen I reverted to the original indexing approach which could be determined at compile time, the compiler statically allocated registers to each array element, which restored the performance.\nTakeaway: Dynamic indexing breaks register allocation.\nKernel Design Patterns In constructing the gemm kernel, we have made the following design decisions:\n2D tiling for data reuse at every level of the memory hierarchy: Shared Memory, Registers, RFC Using a larger warptile to keep the RFC hot Fixing TW to 4 for vectorized and coalesced transactions with global memory Choosing a layout and access pattern for shared memory that avoids bank conflicts and enables intra-warp multicast Autotuning the tile sizes once the kernel is known to exploit the hardware features above We arrived at these high level considerations by beginning at the lowest level of software abstraction (independent operation computed by a single thread) and working our way up to highest level of abstraction (thread block). In doing so, we composed a hierarchical solution that naturally maps to the different levels of hardware abstractions in the GPU.\nMy takeaway is that a similar approach can be used to design kernels for other operations since we are optimizing for the same aspects: data reuse, coalescing, vectorization, shared memory conflicts and multicast.\nReferences Simon Boehm’s worklog Analyzing Modern NVIDIA GPU cores Axel Feldmann’s benchmark ","wordCount":"3641","inLanguage":"en","datePublished":"2026-01-01T09:17:17-06:00","dateModified":"2026-01-01T09:17:17-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://giterator.github.io/blog/posts/gemm/"},"publisher":{"@type":"Organization","name":"Pranav Venkatram","logo":{"@type":"ImageObject","url":"https://giterator.github.io/blog/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://giterator.github.io/blog/ accesskey=h title="Pranav Venkatram (Alt + H)">Pranav Venkatram</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://github.com/giterator/cuda_gemm title=Code><span>Code</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Designing a Performant CUDA GEMM Kernel from First Principles</h1><div class=post-meta><span title='2026-01-01 09:17:17 -0600 CST'>January 1, 2026</span></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#peek-at-final-results aria-label="Peek at Final Results">Peek at Final Results</a></li><li><a href=#aspects-of-a-cuda-kernel aria-label="Aspects of a CUDA Kernel">Aspects of a CUDA Kernel</a></li><li><a href=#designing-the-gemm-kernel aria-label="Designing the GEMM Kernel">Designing the GEMM Kernel</a><ul><li><a href=#identifying-the-independent-operation aria-label="Identifying the independent operation">Identifying the independent operation</a></li><li><a href=#first-thoughts aria-label="First thoughts">First thoughts</a></li><li><a href=#data-reuse aria-label="Data reuse">Data reuse</a></li><li><a href=#warptiling aria-label=Warptiling>Warptiling</a><ul><li><a href=#register-file-cache--instruction-scheduling aria-label="Register File Cache & Instruction Scheduling">Register File Cache & Instruction Scheduling</a></li><li><a href=#organizing-threads-in-a-warp aria-label="Organizing threads in a warp">Organizing threads in a warp</a></li></ul></li><li><a href=#exploiting-shared-memory aria-label="Exploiting Shared Memory">Exploiting Shared Memory</a><ul><li><a href=#shared-memory-layout aria-label="Shared Memory Layout">Shared Memory Layout</a><ul><li><a href=#a-tile-in-shared-memory-bh-x-bk aria-label="A tile in shared memory (BH x BK)">A tile in shared memory (BH x BK)</a></li><li><a href=#b-tile-in-shared-memory-bk-x-bw aria-label="B tile in shared memory (BK x BW)">B tile in shared memory (BK x BW)</a></li></ul></li></ul></li><li><a href=#global-memory-loads-and-stores-coalescing-and-vectorization aria-label="Global Memory loads and stores (Coalescing and Vectorization)">Global Memory loads and stores (Coalescing and Vectorization)</a></li><li><a href=#final-gemm-kernel-stages aria-label="Final GEMM Kernel Stages">Final GEMM Kernel Stages</a></li><li><a href=#autotuning aria-label=Autotuning>Autotuning</a></li></ul></li><li><a href=#interesting-observations aria-label="Interesting Observations">Interesting Observations</a><ul><li><a href=#gemm-on-gpu-and-cpu-are-not-so-different aria-label="GEMM on GPU and CPU are not so different">GEMM on GPU and CPU are not so different</a></li><li><a href=#dynamic-indexing-breaks-register-allocation aria-label="Dynamic Indexing breaks register allocation">Dynamic Indexing breaks register allocation</a></li></ul></li><li><a href=#kernel-design-patterns aria-label="Kernel Design Patterns">Kernel Design Patterns</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In this post, I explore how to implement a performant CUDA GEMM kernel from scratch for NVIDIA GPUs. The goal is to design an efficient kernel grounded in hardware features and first principles, such that its performance characteristics are explainable and achieves performance close to cuBLAS. I examine how the problem naturally decomposes into each level of hardware abstraction (SM, warp, thread) and how to maximize performance at each level through effective use of the memory hierarchy. We start at the lowest level of abstraction (thread) and work our way up to the highest level (SM).</p><p><a href=https://siboehm.com/articles/22/CUDA-MMM>Simon Boehm&rsquo;s worklog</a>
is the primary source of inspiration for this post. My intent is to reproduce his results, document my learnings and extract generic design principles that can be applied to other kernels. While several of these principles such as coalesced, vectorized global memory access and shared memory conflicts are well documented, my main takeaway has been a deeper understanding of warp tiling, particularly with respect to shared memory multicast behavior and register file cache reuse enabled by improved instruction scheduling.</p><h2 id=peek-at-final-results>Peek at Final Results<a hidden class=anchor aria-hidden=true href=#peek-at-final-results>#</a></h2><p>The kernel was run on a Tesla T4 GPU for a variety of matrix sizes and the performance relative to cuBLAS is shown below:</p><table><thead><tr><th style=text-align:center>Matrix Size</th><th style=text-align:center>cuBLAS GFLOPs/s</th><th style=text-align:center>My GFLOPs/s</th><th style=text-align:center>% of cuBLAS Performance Achieved</th></tr></thead><tbody><tr><td style=text-align:center>256</td><td style=text-align:center>697.015</td><td style=text-align:center>118.654</td><td style=text-align:center>17.02</td></tr><tr><td style=text-align:center>512</td><td style=text-align:center>2062.63</td><td style=text-align:center>485.039</td><td style=text-align:center>23.52</td></tr><tr><td style=text-align:center>1024</td><td style=text-align:center>2657.68</td><td style=text-align:center>2662.89</td><td style=text-align:center>100.20</td></tr><tr><td style=text-align:center>2048</td><td style=text-align:center>4447.76</td><td style=text-align:center>3944.75</td><td style=text-align:center>88.69</td></tr><tr><td style=text-align:center>4096</td><td style=text-align:center>4312.22</td><td style=text-align:center>4446.76</td><td style=text-align:center>103.12</td></tr><tr><td style=text-align:center>8192</td><td style=text-align:center>4039.70</td><td style=text-align:center>4151.41</td><td style=text-align:center>102.77</td></tr></tbody></table><p>For larger matrices, our implementation runs faster than cuBLAS on the T4. For smaller matrices, cuBLAS is significantly faster as it selects kernels optimized for smaller problem sizes.</p><h2 id=aspects-of-a-cuda-kernel>Aspects of a CUDA Kernel<a hidden class=anchor aria-hidden=true href=#aspects-of-a-cuda-kernel>#</a></h2><p>A kernel generally comprises of two components:</p><ol><li><p><strong>Compute</strong> (Dot product for GEMM)<br>GPU compute hardware consists of a large number of threads organized in a hierarchical fashion. Each level of hardware has a corresponding software abstraction that executes on it:</p><p><img src=https://giterator.github.io/blog/images/abs.png alt=abs style=display:block;margin-left:auto;margin-right:auto;width:100%></p><p>A kernel is designed to decompose a compute problem in a hierarchical fashion such that each thread computes some portion of the output.<br><br></p></li><li><p><strong>Memory Transactions</strong> (Load and Store)<br>Due to the memory wall, it is essential to hide the latency of loads and stores by overlapping them with compute. Maximizing data reuse at each level of the memory hierarchy mitigates the overall load latency. Data is brought from slower memory to faster memory fewer times and repeatedly accessed from faster memory.<br><br>In a GPU kernel, the loading process follows this procedure:</p><ol><li>Load from Global Memory (DRAM) to Shared Memory (User Managed L1 cache)</li><li>Load from Shared Memory to Registers</li><li>Compute and accumulate result in Registers</li><li>Store from Registers to Global Memory (We don&rsquo;t cache in shared memory since the data won&rsquo;t be reused)</li></ol><p><img src=https://giterator.github.io/blog/images/mem_hie.png alt=mem_hie style=display:block;margin-left:auto;margin-right:auto;width:45%></p><p>Shared Memory is private to each SM and Registers are private to each thread. We can ensure data reuse at the thread block level by loading data into shared memory and reusing it across threads in the block and at the register level by computing multiple outputs per thread (multiple outputs require the same rows/columns).<br><br>L2 cache and Global Memory are accessible across all SMs. Since L2 cache cannot be managed explicitly and the highest level of software abstraction is a thread block (maps to an SM), we will focus on data reuse at the level of shared memory (thread block level) and registers (thread level).</p></li></ol><h2 id=designing-the-gemm-kernel>Designing the GEMM Kernel<a hidden class=anchor aria-hidden=true href=#designing-the-gemm-kernel>#</a></h2><p style=font-size:1.2em;text-align:center>C<sub>m,n</sub> = A<sub>m,k</sub> x B<sub>k,n</sub></p><h3 id=identifying-the-independent-operation>Identifying the independent operation<a hidden class=anchor aria-hidden=true href=#identifying-the-independent-operation>#</a></h3><p>Each output element is computed by performing a dot product of a row from matrix A and a column from matrix B i.e. each element in C is computed independently of others.</p><p><img src=https://giterator.github.io/blog/images/dot_prod.png alt=dot_prod style=display:block;margin-left:auto;margin-right:auto;width:60%></p><h3 id=first-thoughts>First thoughts<a hidden class=anchor aria-hidden=true href=#first-thoughts>#</a></h3><p>Since each element of C is computed independently, we can have each output element computed by a different thread. This would involve each thread loading 1 row of A and 1 column of B from Global memory and computing the dot product.</p><p>However, due to the memory wall, particularly for large matrices, the latency of loading data from Global memory outweighs the compute time. Further, output elements residing in the same row require the same row of A and different columns of B. Likewise, output elements residing in the same column require the same column of B and different rows of A. The current approach involves reloading the same data for computing multiple outputs.</p><p>Memory latency can be mitigated by loading data once and reusing it to compute multiple outputs.</p><h3 id=data-reuse>Data reuse<a hidden class=anchor aria-hidden=true href=#data-reuse>#</a></h3><p>As mentioned earlier, the GPU memory hierarchy offers 2 levels of caching we can control: Shared Memory (L1) and Registers.</p><p>Since Registers are the fastest memory available, we should try to maximize data reuse at this level. As registers are private to each thread, we can only achieve data reuse at the register level by computing multiple outputs per thread. Each thread is thus responsible for computing a <em>tile</em> of the output matrix by reusing rows of A and columns of B cached in the registers.</p><p>There are 2 approaches to tiling:</p><ol><li>1D output tile</li><li>2D output tile</li></ol><p>Let&rsquo;s assume that n = m = k i.e. matrices A, B and C are square matrices of size n x n.</p><p><img src=https://giterator.github.io/blog/images/tiling.png alt=tiling style=display:block;margin-left:auto;margin-right:auto;width:100%></p><p>As shown above, a square 2D tile enables maximum data reuse. In general, a rectangular tile enables more reuse than a 1D tile but lesser reuse than a square tile.</p><p>The takeaway from this analysis is that we should use a 2D tile but the exact dimensions can be treated as parameters that can be tuned. Further, the number of FLOPs needed to compute a single output element is fixed (2n). Maximizing data reuse allows us to compute the same number of FLOPs with fewer loads, mitigating memory latency (greater arithemtic intensity).</p><blockquote><p><u><strong>Current state of the kernel:</strong></u></p><p>Each thread computes a 2D output tile</p></blockquote><p>Now that we have determined what the lowest level of hardware abstraction should compute, we move to defining the higher levels of abstraction.</p><p>Threads that compute tiles residing in the same row or the same column load the same rows of A or same columns of B from Global memory into their registers. These are redundant loads that stress the memory bus and waste bandwidth. Hence, we cache data in shared memory (L1) and reuse it across threads in the same thread block.</p><p>Since we have determined that a 2D tile maximizes data reuse, the tile computed at each level of the software abstraction should be a 2D tile where tiles computed by lower levels of abstraction compose a larger tile at a higher level.</p><p>We define the terminology as follows:</p><ol><li>Threadtile: tile computed by 1 thread</li><li>Warptile: tile computed by 1 warp (32 threads)</li><li>Blocktile: tile computed by 1 thread block (multiple warps)</li></ol><p>Each of these tiles will have their own heights and widths. We use the following variable names for simplicity:</p><ol><li>Threadtile (height, width): TH x TW</li><li>Warptile (height, width): WH x WW</li><li>Blocktile (height, width): BH x BW</li></ol><p><img src=https://giterator.github.io/blog/images/hierarchical.png alt=hierarchical style=display:block;margin-left:auto;margin-right:auto;width:50%></p><h3 id=warptiling>Warptiling<a hidden class=anchor aria-hidden=true href=#warptiling>#</a></h3><p>In hardware, a warp (group of 32 threads) is the fundamental execution unit. Before defining how threads in a warp are organized to compute a Warptile, we should first explore a key warp-specific hardware feature: Register File Cache (RFC).</p><h4 id=register-file-cache--instruction-scheduling>Register File Cache & Instruction Scheduling<a hidden class=anchor aria-hidden=true href=#register-file-cache--instruction-scheduling>#</a></h4><p>There is little official documentation on the Register File Cache. Interestingly, researchers have reverse engineered GPU architectures, shedding light on these details. I referred to <a href=https://arxiv.org/pdf/2503.20481>Analyzing Modern NVIDIA GPU cores</a> to understand the RFC and instruction scheduling, which collectively seem to provide a motivation for warptiling.</p><p>In GPUs, the Register Files are much larger than the L1 cache. Since threads access registers repeatedly, there is a great deal of contention for the register file, resulting in stalls due to serialized access. As such, the GPU hardware contains a Register File Cache (RFC) that caches the most recently used registers. This exploits temporal locality such that repeated access of the same registers avoids going to the register file.</p><p>As seen in the diagram below, the RFC is island-specific i.e. register caching can only be exploited at the warp level (Note: the diagram refers to islands as sub-cores).</p><figure style=text-align:center><img src=https://giterator.github.io/blog/images/sm_arch_label.png alt="SM Architecture: RFC is island-specific."><figcaption><em>SM Architecture (Modified from: <a href=https://arxiv.org/pdf/2503.20481>Analyzing Modern NVIDIA GPU cores</a>)</em></figcaption></figure><p>From a scheduling point of view, the warp scheduler implements Fine Grained Multi Threading i.e. it determines which warp should issue an instruction every cycle. The scheduler prioritizes instructions from the existing warp provided it is ready to execute. If the current warp is stalled, the scheduler will switch to another warp to issue instructions and keep the execution units busy.</p><p>Considering the RFC and scheduler behaviour, a context switch to a different warp would cause a different set of registers (containing rows of A and columns of B) to be accessed, invalidating the RFC. Hence, to exploit this level of the memory hierarchy, we should try to keep the same warp active for as long as possible. To achieve this, the instruction buffer must contain an independent instruction from the same warp i.e. can be issued in the current cycle.</p><h4 id=organizing-threads-in-a-warp>Organizing threads in a warp<a hidden class=anchor aria-hidden=true href=#organizing-threads-in-a-warp>#</a></h4><p>Having established the role of an individual thread and the hardware supporting warp execution, we can now define how threads in a warp are organized to compute a Warptile.</p><p>Since each thread computes a TH x TW tile of C, a warp will compute 32 such tiles. At the thread level, we computed a 2D threadtile to enable data reuse at the register level. At the warp level, we organize these threadtiles in a 2D manner once again to enable reuse at the shared memory level. Let&rsquo;s assume that the data to compute the blocktile is already loaded into shared memory. This 2D organization enables fewer rows and columns to be loaded from shared memory into the registers at a warp level since each thread now requires the same rows and columns as others. Since threads in a warp execute in lock-step, threads loading the same elements from shared memory will do so at the same time. GPU hardware is optimized to service such requests via <strong>Intra-warp multicast</strong> wherein a shared memory element is accessed once and sent to all requesting threads in the warp in the same time it would take for a single thread to load a single element. This effectively reduces the number of loads from shared memory to the registers at a warp level. <a href=https://feldmann.nyc/blog/smem-microbenchmarks>Axel Feldmann&rsquo;s wonderful benchmark</a> showcases this among many others experiments.</p><p><img src=https://giterator.github.io/blog/images/multicast.png alt=multicast style=display:block;margin-left:auto;margin-right:auto;width:70%></p><p>We also want to ensure that a warp doesn&rsquo;t get context switched and invalidate the RFC so we have a warp compute a WH x WW warptile that is larger than 32 threadtiles. A warp will initially compute a warp subtile (32 thread tiles) and then stride to compute the remaining warp subtiles within the same WH x WW warptile. This increases the amount of work done by a warp, thus increasing the warp level ILP. This ensures that the warp scheduler has instructions from the same warp to issue in subsequent cycles, keeping the RFC hot. Larger warptile sizes increase the amount of sequential work done by a warp. In the extreme case where the warptile size is too large, we will exceed register limits and perform too much sequential work, leaving SMs idle. In the other extreme, the warptile size is too small, resulting in frequent invalidations of the RFC. The warptile size must be chosen to strike a balance between these two considerations (done by autotuning).</p><p><img src=https://giterator.github.io/blog/images/warptiling.png alt=warptiling style=display:block;margin-left:auto;margin-right:auto;width:100%></p><p>The warptiles computed by each warp in a thread block compose a blocktile. The warptiles are organized in a 2D manner as well, resulting in a BH x BW blocktile.</p><blockquote><p><u><strong>Current state of the kernel:</strong></u></p><p>Warptiling exploits <strong>two distinct hardware features</strong> using <strong>two levels of tiling</strong>:</p><p><strong>1. Warp subtile (32 thread tiles)</strong><br>The 32 threadtiles are organized in a 2D manner, leveraging <strong>intra-warp multicast</strong>.</p><p><strong>2. Warptile (multiple warp subtiles)</strong><br>Striding to compute remaining warp subtiles within a larger warptile <strong>increases</strong> <strong>warp level ILP</strong>, keeping the <strong>RFC hot</strong>.</p></blockquote><h3 id=exploiting-shared-memory>Exploiting Shared Memory<a hidden class=anchor aria-hidden=true href=#exploiting-shared-memory>#</a></h3><p>Since shared memory is private to an SM, data can only be shared by threads in the same thread block. To compute a blocktile of C, the necessary rows of A and columns of B are loaded from global to shared memory. However, since the K dimension can be large, BH rows of A and BW columns of B will exceed the capacity of shared memory. Hence, we need to tile the inputs along the K dimension. The operation computed by the kernel is now a tiled inner product where each dot product is chunked and the partial results are iteratively accumulated.</p><p>The tiling approach for the inputs is now extended from:
<img src=https://giterator.github.io/blog/images/old_compute.png alt=old_compute style=display:block;margin-left:auto;margin-right:auto;width:70%></p><p>to:
<img src=https://giterator.github.io/blog/images/new_compute.png alt=new_compute style=display:block;margin-left:auto;margin-right:auto;width:80%></p><h4 id=shared-memory-layout>Shared Memory Layout<a hidden class=anchor aria-hidden=true href=#shared-memory-layout>#</a></h4><p>The BH x BK tile of A and BW x BK tile of B are loaded from global memory into shared memory. The BH x BW tile of C is accumulated into registers iteratively and then stored to global memory. Each output element requires 1 row of BK elements from the tile of A and 1 column of BK elements from the tile of B. Hence, each threadtile will load multiple rows and columns of A and B into its registers.</p><p>Shared memory is organized into 32 banks where each element in a bank stores 4 bytes. Since we are operating on FP32 values, each element stores a single FP32 value. When reading from or writing to shared memory, if threads within the same warp access different elements in the same bank, a bank conflict will occur, resulting in serialized access. However, if threads within the same warp access the same element in the same bank, the element is accessed once and broadcasted so there is no performance penalty. Based on these 2 factors, we can design our data layout and access pattern to minimize bank conflicts and maximize intra warp multicast.</p><p>An array of floats is mapped to banks in shared memory as follows:
<img src=https://giterator.github.io/blog/images/smem.png alt=smem style=display:block;margin-left:auto;margin-right:auto;width:75%></p><p>We use 2 arrays in shared memory to store the tiles of A and B respectively.</p><h5 id=a-tile-in-shared-memory-bh-x-bk>A tile in shared memory (BH x BK)<a hidden class=anchor aria-hidden=true href=#a-tile-in-shared-memory-bh-x-bk>#</a></h5><p>For reasoning about the layout of A in shared memory banks, we assume that each column maps to the same bank i.e. BK is a multiple of 32. This is an oversimplification but it is helpful to reason about conflicts.</p><p>If the BH x BK tile of A is stored in <strong>row major order</strong> in shared memory, threads in a warp which need the same rows will exploit intra-warp multicast. However, threads that read different rows will conflict with each other as shown below:</p><p><img src=https://giterator.github.io/blog/images/a_row_maj.png alt=a_row_maj style=display:block;margin-left:auto;margin-right:auto;width:80%></p><p>This can be solved by storing the tile of A in <strong>column major order</strong> (transposed layout). Threads in a warp accessing the same column will still exploit multicast as before but threads accessing different columns will now access different banks, avoiding bank conflicts:</p><p><img src=https://giterator.github.io/blog/images/a_col_maj.png alt=a_col_maj style=display:block;margin-left:auto;margin-right:auto;width:80%></p><h5 id=b-tile-in-shared-memory-bk-x-bw>B tile in shared memory (BK x BW)<a hidden class=anchor aria-hidden=true href=#b-tile-in-shared-memory-bk-x-bw>#</a></h5><p>As with A, we assume that each column maps to the same bank i.e. BW is a multiple of 32. If we store the tile of B in <strong>row major order</strong>, threads in a warp which need the same columns of B will exploit intra-warp multicast. Threads that need different columns of B will access different bank, avoiding conflicts anyway.</p><p><img src=https://giterator.github.io/blog/images/b_row_maj.png alt=b_row_maj style=display:block;margin-left:auto;margin-right:auto;width:80%></p><h3 id=global-memory-loads-and-stores-coalescing-and-vectorization>Global Memory loads and stores (Coalescing and Vectorization)<a hidden class=anchor aria-hidden=true href=#global-memory-loads-and-stores-coalescing-and-vectorization>#</a></h3><p>Now that we have defined the compute and desired shared memory layout, we can define the approach to load the tiles of A and B from global memory into shared memory.</p><p>Even though multiple blocktiles use the same rows/columns as others, we are forced to load them independently within each thread block. The data may reside in the L2 cache, enabling faster loads for subsequent blocks although the order of thread block execution is abstracted away from the programmer so we cannot assume this is the case.</p><p>Each thread block will load a BH x BK tile of A and a BK x BW tile of B from global memory into shared memory. It is beneficial for warps to load contiguous data from global memory as the loads are coalesced into fewer transactions by the hardware. This also applies to stores.</p><p>Note: Coalescing is a warp level optimization.</p><p>Further, vectorized load and store instructions improve performance by reducing the instruction issue overhead as fewer load/store instructions are executed. 4-wide vectorized instructions enable 4 floats to be loaded per instruction. At a hardware level, this still takes the same number of cycles as loading 1 float per instruction (shown in <a href=https://feldmann.nyc/blog/smem-microbenchmarks>Axel Feldmann&rsquo;s benchmark</a>).</p><p>To exploit both coalescing and 4-wide vectorized loads, each thread in a warp loads 4 contiguous floats from global memory (A and B arrays). Hence, the threadtile width (TW) must be 4. All threads in the thread block then stride vertically to load the remaining elements from A and B tiles. The loaded elements are placed in the desired layout (row major or tranposed) in shared memory.</p><p><img src=https://giterator.github.io/blog/images/gmem.png alt=gmem style=display:block;margin-left:auto;margin-right:auto;width:80%></p><p>Once a thread has computed a threadtile, it writes the values back to global memory. Since we have organized threads in a warp to compute a 2D warptile and TW=4, each thread writes 4 contiguous elements of C to global memory and threadtiles on the same row have their stores coalesced.</p><h3 id=final-gemm-kernel-stages>Final GEMM Kernel Stages<a hidden class=anchor aria-hidden=true href=#final-gemm-kernel-stages>#</a></h3><p>The final kernel implementation has specific stages separated by barriers where each thread plays a different role in each stage:</p><p><img src=https://giterator.github.io/blog/images/stages.png alt=stages style=display:block;margin-left:auto;margin-right:auto;width:80%></p><p>Since threads cooperate within a thread block, the barriers ensure that all threads in a block reach the barrier before proceeding to the next stage.</p><h3 id=autotuning>Autotuning<a hidden class=anchor aria-hidden=true href=#autotuning>#</a></h3><p>The hierarchical kernel exposes parameters at each level of abstraction:</p><ol><li>Threadtile dimensions: TH (TW=4 already)</li><li>Warptile dimensions: WH, WW</li><li>Blocktile dimensions: BH, BW</li><li>Tile size along reduction dimension (inputs): BK</li></ol><p>I ran an autotuning script that exhaustively sweeps over a range of values for each parameter and benchmarks the performance of each configuration. The configuration with the best performance was selected.</p><p>Typically, we would like to write a generic kernel that can be used across GPU architectures rather than rewriting kernels every time a new architecture is released. Our gemm implementation is sufficiently generic that it can be deployed across different GPUs since we exploit common features supported by NVIDIA GPUs (provided the architecture is not vastly different). However, since different architectures have different memory bandwidths, cache sizes etc. the optimal tile sizes vary. Autotuning helps us treat the kernel as a black box and find the optimal tile sizes without having to analyze architectural differences.</p><h2 id=interesting-observations>Interesting Observations<a hidden class=anchor aria-hidden=true href=#interesting-observations>#</a></h2><h3 id=gemm-on-gpu-and-cpu-are-not-so-different>GEMM on GPU and CPU are not so different<a hidden class=anchor aria-hidden=true href=#gemm-on-gpu-and-cpu-are-not-so-different>#</a></h3><p>As mentioned at the start of the post, a kernel consists of compute and memory transactions. In the fully implemented GEMM kernel of ~130 lines of code, the actual compute portion is only 1 line of code wrapped in 4 loops (totally ~10 lines of code). The bulk of the code handles data loading, caching in shared memory, populating registers and storing the result i.e. data movement.</p><p>Since the lowest level of GPU hardware is a thread, the compute code is exactly the same as that of blocked matrix multiplication for single threaded CPU execution:</p><pre tabindex=0><code>for(int c = 0; c &lt; w_hor; c++){
    for(int r = 0; r &lt; w_vert; r++){
        for(int tr = 0; tr &lt; TH; tr++){
            for(int tc = 0; tc &lt; TW; tc++){
                c_out[(r*TH + tr)*(w_hor*TW) + (c*TW + tc)] += reg_a[r*TH+tr] * reg_b[c*TW+tc];
            }
        }
    }
}
</code></pre><p>On a GPU, this is scaled up to compute a larger output matrix using a larger number of threads.</p><h3 id=dynamic-indexing-breaks-register-allocation>Dynamic Indexing breaks register allocation<a hidden class=anchor aria-hidden=true href=#dynamic-indexing-breaks-register-allocation>#</a></h3><p>The autotuned parameters resulted in some bank conflicts in shared memory. I resolved the conflicts through a combination of padding and modified access pattern. Despite conflict resolution, the kernel was still slower than the original version so I don&rsquo;t cover it in this post. In the process of playing around with the access pattern, I came across an interesting compiler related nuance.</p><p>I was loading elements from shared memory into an array that was small enough to fit into registers. Since the array size is known at compile time, I assumed that the compiler would assign registers to the array elements. It turns out that my modified indexing logic which utilizes a % operator causes the index to be determined at runtime. Even though the index always falls within the bounds of the array, this cannot be guaranteed at compile time. The compiler plays it safe and allocates the array in local memory, resulting in a ~20x drop in performance. This is effectively register spilling even though the array fits in registers and the index is always within bounds.</p><p>Inspecting the PTX, we see store instructions to local memory:</p><p><img src=https://giterator.github.io/blog/images/ptx.png alt=ptx style=display:block;margin-left:auto;margin-right:auto;width:30%></p><p>When I reverted to the original indexing approach which could be determined at compile time, the compiler statically allocated registers to each array element, which restored the performance.</p><blockquote><p><strong>Takeaway:</strong> Dynamic indexing breaks register allocation.</p></blockquote><h2 id=kernel-design-patterns>Kernel Design Patterns<a hidden class=anchor aria-hidden=true href=#kernel-design-patterns>#</a></h2><p>In constructing the gemm kernel, we have made the following design decisions:</p><ol><li>2D tiling for data reuse at every level of the memory hierarchy: Shared Memory, Registers, RFC</li><li>Using a larger warptile to keep the RFC hot</li><li>Fixing TW to 4 for vectorized and coalesced transactions with global memory</li><li>Choosing a layout and access pattern for shared memory that avoids bank conflicts and enables intra-warp multicast</li><li>Autotuning the tile sizes once the kernel is known to exploit the hardware features above</li></ol><p>We arrived at these high level considerations by beginning at the lowest level of software abstraction (independent operation computed by a single thread) and working our way up to highest level of abstraction (thread block). In doing so, we composed a hierarchical solution that naturally maps to the different levels of hardware abstractions in the GPU.</p><p>My takeaway is that a similar approach can be used to design kernels for other operations since we are optimizing for the same aspects: data reuse, coalescing, vectorization, shared memory conflicts and multicast.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li><a href=https://siboehm.com/articles/22/CUDA-MMM>Simon Boehm&rsquo;s worklog</a></li><li><a href=https://arxiv.org/pdf/2503.20481>Analyzing Modern NVIDIA GPU cores</a></li><li><a href=https://feldmann.nyc/blog/smem-microbenchmarks>Axel Feldmann&rsquo;s benchmark</a></li></ol></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://giterator.github.io/blog/>Pranav Venkatram</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>